{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emirunlu26/qwen3-4B-finetuned-turkish-youtube-text-summarizer/blob/main/finetune_text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsIMPqhvWKpg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes transformers peft accelerate scipy einops evaluate trl rouge_score"
      ],
      "metadata": {
        "collapsed": true,
        "id": "psqCsAhfgJfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets huggingface_hub fsspec"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FHVWli7NW3lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjPUiwq_Ko5"
      },
      "source": [
        "**LOAD DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"emirunlu26/turkish-youtube-text-summarization\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pgVaBTFVTjMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FINE-TUNE QWEN3-4B ON Turkish Youtube Text Summarization**"
      ],
      "metadata": {
        "id": "1sE2WURqTjtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    GenerationConfig,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xCmMGy9RgufY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SET CONFIGURATIONS FOR 4-BIT QUANTIZATION OF THE BASE MODEL\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='nf4',\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "    )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0nv5iTGTgTZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD TOKENIZER AND 4-BIT QUANTIZED VERSION OF BASE MODEL\n",
        "model_name = \"Qwen/Qwen3-4B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\",\n",
        "    quantization_config=bnb_config\n",
        ")"
      ],
      "metadata": {
        "id": "I7PdmLKXZsHH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERAT FORMATTED PROMPTS FROM SAMPLES\n",
        "def generate_prompts(samples):\n",
        "  instruction_prompt = \"Bu Youtube videosunu, ana teması ve önemli noktalarına odaklanarak kısa ama öz ve soyutlayıcı bir şekilde özetle (abstractive summary):\\n\"\n",
        "\n",
        "  titles = samples[\"title\"]\n",
        "  categories = samples[\"category\"]\n",
        "  channels = samples[\"channel\"]\n",
        "  texts = samples[\"text\"]\n",
        "\n",
        "  prompts = list()\n",
        "  for title,category,channel,text in zip(titles,categories,channels,texts):\n",
        "    data_prompt = f\"Başlık: {title}\\n\" \\\n",
        "  + f\"Kategori: {category}\\n\" \\\n",
        "  + f\"Kanal: {channel}\\n\" \\\n",
        "  + f\"Metin: {text}\"\n",
        "    prompts.append(instruction_prompt + data_prompt)\n",
        "\n",
        "  return {\"prompt\":prompts}"
      ],
      "metadata": {
        "id": "j1KCBPOKZPjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLY CHAT TEMPLATE TO EACH GENERATED PROMPT\n",
        "def apply_chat_template(samples):\n",
        "  def apply_to_prompt(prompt):\n",
        "    messages = [{\"role\":\"user\",\"content\":prompt}]\n",
        "    chat_template = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False\n",
        "    )\n",
        "    return chat_template\n",
        "  prompts = samples[\"prompt\"]\n",
        "  chat_templates = list(map(apply_to_prompt,prompts))\n",
        "  return {\"prompt\":chat_templates}"
      ],
      "metadata": {
        "id": "T8rGMiGTc4VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # PREPROCESS SAMPLES\n",
        "def preprocess_samples(samples):\n",
        "  prompts = samples[\"prompt\"]\n",
        "  model_inputs = tokenizer(prompts,return_tensors=\"pt\",padding=\"max_length\",truncation=True,max_length=8000)\n",
        "  outputs = samples[\"summary\"]\n",
        "  labels = tokenizer(outputs,return_tensors=\"pt\",padding=\"max_length\",truncation=True,max_length=8000)\n",
        "  # SET PADDING TOKENS TO -100 IN ORDER TO MASK THEM. OTHERWISE THE MODEL OVERFITS TO GENERATE PADDING TOKENS\n",
        "  model_inputs[\"labels\"] = [\n",
        "      [(label_id if label_id != tokenizer.pad_token_id else -100) for label_id in label_ids]\n",
        "      for label_ids in labels[\"input_ids\"]\n",
        "  ]\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "rOX5xQAyZd4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLY PREPROCESSING TO SAMPLES IN THE DATASET\n",
        "dataset = dataset.map(generate_prompts,batched=True)\n",
        "dataset = dataset.map(apply_chat_template,batched=True)\n",
        "dataset = dataset.map(preprocess_samples,batched=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h8Pai-FlZxaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# CREATE PARAMETER-EFFICIENT MODEL FROM THE BASE MODEL USING LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=32, #Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        'q_proj',\n",
        "        'k_proj',\n",
        "        'v_proj',\n",
        "        'dense'\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "pvG3UEm6gH5a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE WHICH METRICS ARE CALCULATED DURING TRAINING (NOT USED DURING TRAINING DUE TO MEMORY CONSTRAINTS)\n",
        "def build_compute_metrics(tokenizer):\n",
        "  def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    tokenizer.batch_decode()\n",
        "    ref_summary = tokenizer.decode(labels,skip_special_tokens=True).strip(\"\\n\")\n",
        "    candidate_summary = tokenizer.decode(preds,skip_special_tokens=True).strip(\"\\n\")\n",
        "    score = scorer.score(ref_summary, candidate_summary)\n",
        "    return modify_rouge_score(score)\n",
        "\n",
        "  return compute_metrics"
      ],
      "metadata": {
        "id": "hL1__DbmtW3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "3M7D1-J3WyVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE TRAINING ARGUMENTS. BATCH SIZE SET TO 2 IN ORDER TO REDUCE GPU MEMORY USAGE\n",
        "batch_size = 2\n",
        "logging_steps = 50\n",
        "output_dir = \"qwen3-4B-finetuned-turkish-youtube-text-summarizer\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs =2,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.1,\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_steps=logging_steps,\n",
        "    fp16=True,\n",
        "    push_to_hub=True\n",
        ")\n",
        "\n",
        "# DEFINE TRAINER\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    compute_metrics=None, # Set to None to reduce memory usage of GPU\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "tFZstF_L-0bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINE-TUNE THE MODEL\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "5vVKomXzOwkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE THE MODEL\n",
        "trainer.save_model(output_dir)"
      ],
      "metadata": {
        "id": "cnNtlHqrCqa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATE MODEL ON TEST SET\n",
        "trainer.evaluate(eval_dataset=dataset[\"test\"])"
      ],
      "metadata": {
        "id": "h1Mbtpbiu4os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TESTING FINETUNED MODEL - CALCULATING ROUGE-1, ROUGE-2 AND ROUGE-L F1 SCORE**"
      ],
      "metadata": {
        "id": "El8cDoLoZmpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base_model_name = \"Qwen/Qwen3-4B\"\n",
        "adapter_model_name = \"emirunlu26/qwen3-4B-finetuned-turkish-youtube-text-summarizer\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_name,device_map=\"cuda\",torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "model = PeftModel.from_pretrained(base_model, adapter_model_name).to(\"cuda\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "EduZgbGKZ6pS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(sample):\n",
        "  instruction_prompt = \"Bu Youtube videosunu, ana teması ve önemli noktalarına odaklanarak kısa ama öz ve soyutlayıcı bir şekilde özetle (abstractive summary):\\n\"\n",
        "\n",
        "  title = sample[\"title\"]\n",
        "  category = sample[\"category\"]\n",
        "  channel = sample[\"channel\"]\n",
        "  text = sample[\"text\"]\n",
        "\n",
        "  data_prompt = f\"Başlık: {title}\\n\" \\\n",
        "  + f\"Kategori: {category}\\n\" \\\n",
        "  + f\"Kanal: {channel}\\n\" \\\n",
        "  + f\"Metin: {text}\"\n",
        "  return (instruction_prompt + data_prompt)"
      ],
      "metadata": {
        "id": "QhhF8_z8aAdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(model,model_input):\n",
        "  generated_ids = model.generate(\n",
        "      **model_input,\n",
        "      max_new_tokens=2000\n",
        "      )\n",
        "  output_ids = generated_ids[0][len(model_input.input_ids[0]):].tolist()\n",
        "  summary = tokenizer.decode(output_ids,skip_special_tokens=True).strip(\"\\n\")\n",
        "  return summary"
      ],
      "metadata": {
        "id": "qjWFQkTSa8ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sample(sample):\n",
        "  prompt = generate_prompt(sample)\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=False,\n",
        "      add_generation_prompt=True,\n",
        "      enable_thinking=False\n",
        "  )\n",
        "  model_input = tokenizer([text],return_tensors=\"pt\").to(model.device)\n",
        "  return model_input"
      ],
      "metadata": {
        "id": "czhiqTsWaLj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = dataset[\"test\"]\n",
        "test_samples = list(map(preprocess_sample,test_set))\n",
        "summaries = list()\n",
        "\n",
        "for index,sample in enumerate(test_samples):\n",
        "  candidate_summary = generate_summary(model,sample)\n",
        "  print(f\"{index+1}. özet hazırlandı.\")\n",
        "  summaries.append((test_set[index][\"summary\"],candidate_summary))"
      ],
      "metadata": {
        "id": "SVBgI556b7Ro",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
      ],
      "metadata": {
        "id": "myoLJkpoepcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_rouge_score(score):\n",
        "  return {key: score[key].fmeasure for key in score}"
      ],
      "metadata": {
        "id": "CxYBEPwS4y13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "rouge_scores = list(map(lambda s: scorer.score(s[0],s[1]),summaries))\n",
        "rouge_scores = list(map(modify_rouge_score,rouge_scores))\n",
        "\n",
        "rouge_1_scores = [score[\"rouge1\"] for score in rouge_scores]\n",
        "rouge_1_scores = np.array(rouge_1_scores)\n",
        "\n",
        "rouge_2_scores = [score[\"rouge2\"] for score in rouge_scores]\n",
        "rouge_2_scores = np.array(rouge_2_scores)\n",
        "\n",
        "rouge_l_scores = [score[\"rougeL\"] for score in rouge_scores]\n",
        "rouge_2_scores = np.array(rouge_l_scores)\n",
        "\n",
        "average_result = {\n",
        "    \"rouge1\": round(float(np.mean(rouge_1_scores)),2),\n",
        "    \"rouge2\": round(float(np.mean(rouge_2_scores)),2),\n",
        "    \"rougeL\": round(float(np.mean(rouge_l_scores)),2)\n",
        "}"
      ],
      "metadata": {
        "id": "oorQBApme_DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(average_result)"
      ],
      "metadata": {
        "id": "1lPvE3GJ91iN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPx8fGFxaVwb1gCoCrEeljF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}